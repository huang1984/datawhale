{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词\n",
    "#### 最大匹配法【Maximum Matching】MM\n",
    "最大匹配是指以词典为依据，取词典中最长单词的字数量作为截取词的起始匹配长度，将截取后的最大长度的词与词典中的词进行比对（为提升扫描效率，还可以根据字数多少设计多个字典，然后根据字数分别从不同字典中进行扫描匹配）， 直到还剩一个单字则终止，如果该单字无法切分，则作为未登录词处理（没有被收录在分词词表中但必须切分出来的词，包括各类专有名词（人名、地名、企业名等）、缩写词、新增词汇等等）\n",
    "\n",
    "栗子：词典中最长词为“中华人民共和国”共7个汉字，则最大匹配起始字数为7个汉字。然后逐字递减，重新在对应的词典中循环比对\n",
    "\n",
    "#### 正向最大匹配法\n",
    "从左往右地进行最大匹配法。尽可能地选择与词典中最长单词匹配的词作为目标分词，然后进行下一次匹配\n",
    "\n",
    "**栗子**\n",
    "待切分文本 计算语言学课程有意思\n",
    "词典（表） {\"计算\", \"计算语言学\", \"课程\", \"有\", \"意思\"}(真实的词表中会有成千上万个平时我们使用的已经分好的词语)\n",
    "\n",
    "**匹配过程**\n",
    "确定最大匹配的起始子串字数为词典中最长单词的长度5\n",
    "输入 计算语言学课程有意思\n",
    "第一轮 取子串“计算语言学”，正向取词，如果匹配失败，每次去掉待匹配子串最后面的一个字\n",
    "第1次. “计算语言学”，扫描词典表，匹配，输出“计算语言学”，输入变为“课程有意思”\n",
    "第二轮 取子串“课程有意思”\n",
    "第1次. “课程有意思”， 扫描词典表，不匹配，子串长度减一变为“课程有意”\n",
    "第2次. “课程有意”， 扫描词典表，不匹配，子串长度减一变为“课程有”\n",
    "第3次. “课程有”， 扫描词典表，不匹配，子串长度减一变为“课程”\n",
    "第4次. “课程”， 扫描词典表，匹配，输出“课程”，输入变为“有意思”\n",
    "第三轮 取子串“有意思”\n",
    "第1次. “有意思”， 扫描词典表，不匹配，子串长度减一变为“有意”\n",
    "第2次. “有意”， 扫描词典表，不匹配，子串长度减一变为“有”\n",
    "第3次. “有”， 扫描词典表，匹配，输出“有”，输入变为“意思”\n",
    "第四轮 取子串“意思”\n",
    "第1次. “意思”，扫描词典表，匹配，输出“意思”，输入变为“”\n",
    "输入长度为零，终止扫描。\n",
    "最终分词结果为：计算语言学/课程/有/意思\n",
    "\n",
    "#### 逆向最大匹配算法 RMM\n",
    "从右往左地进行最大匹配法。尽可能地选择与词典中最长单词匹配的词作为目标分词，然后进行下一次匹配。在实践中，逆向最大匹配算法性能通常优于正向最大匹配算法。\n",
    "\n",
    "**栗子：**\n",
    "待切分文本 计算语言学课程有意思\n",
    "词典（表） {\"计算\", \"计算语言学\", \"课程\", \"有\", \"意思\"}(真实的词表中会有成千上万个平时我们使用的已经分好的词语)\n",
    "\n",
    "**匹配过程**\n",
    "确定最大匹配的起始子串字数为词典中最长单词的长度5\n",
    "\n",
    "输入 计算语言学课程有意思\n",
    "\n",
    "第一轮 取子串“课程有意思”，逆向取词，如果匹配失败，每次去掉待匹配子串最前面的一个字\n",
    "第1次. “课程有意思”，扫描词典表，不匹配，子串长度减一变为“程有意思”\n",
    "第2次. “程有意思”，扫描词典表，不匹配，子串长度减一变为“有意思”\n",
    "第3次. “有意思”，扫描词典表，不匹配，子串长度减一变为“意思”\n",
    "第4次. “意思”，扫描词典表，匹配，输出“意思”，输入变为“计算语言学课程有”\n",
    "\n",
    "第二轮 取子串“言学课程有”\n",
    "第1次. “言学课程有”， 扫描词典表，不匹配，子串长度减一变为“学课程有”\n",
    "第2次. “学课程有”， 扫描词典表，不匹配，子串长度减一变为“课程有”\n",
    "第3次. “课程有”， 扫描词典表，不匹配，子串长度减一变为“程有”\n",
    "第4次. “程有”， 扫描词典表，子串长度减一变为“有”\n",
    "第5次. “有”， 扫描词典表，匹配，输出“有”，输入变为“计算语言学课程”\n",
    "\n",
    "第三轮 取子串“语言学课程”\n",
    "第1次. “语言学课程”， 扫描词典表，不匹配，子串长度减一变为“言学课程”\n",
    "第2次. “言学课程”， 扫描词典表，不匹配，子串长度减一变为“学课程”\n",
    "第3次. “学课程”， 扫描词典表，不匹配，子串长度减一变为“课程”\n",
    "第4次. “课程”， 扫描词典表，匹配，输出“课程”，输入变为“计算语言学”\n",
    "\n",
    "第四轮 取子串“计算语言学”\n",
    "第1次. “计算语言学”，扫描词典表，匹配，输出“计算语言学”，输入变为“”\n",
    "输入长度为零，终止扫描。\n",
    "最终分词结果为：计算语言学/课程/有/意思\n",
    "\n",
    "#### 双向最大匹配法\n",
    "两种算法都切一遍，然后根据大颗粒度词越多越好，非词典词(未登录词)和单字词越少越好的原则，选取其中一种分词结果输出\n",
    "\n",
    "**算法流程：**\n",
    "比较正向最大匹配和逆向最大匹配结果\n",
    "如果分词数量结果不同，那么取分词数量较少的那个\n",
    "如果分词数量结果相同\n",
    "3.1 分词结果相同，可以返回任何一个\n",
    "3.2 分词结果不同，返回单字数比较少的那个\n",
    "\n",
    "参考：https://www.cnblogs.com/Jm-15/p/9403352.html\n",
    "\n",
    "\n",
    "\n",
    "语言模型\n",
    "\n",
    "unigram: 单word\n",
    "\n",
    "bigram: 双word\n",
    "\n",
    "trigram: 3 word\n",
    "\n",
    "**栗子**\n",
    "\n",
    "西安交通大学：\n",
    "\n",
    "unigram形式为：西/安/交/通/大/学\n",
    "\n",
    "bigram形式为： 西安/安交/交通/通大/大学\n",
    "\n",
    "trigram形式为：西安交/安交通/交通大/通大学"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词处理流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba as jb\n",
    "from collections import Counter #统计词频\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow.contrib.keras as kr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载停用词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwordslist(stopfile):\n",
    "    stopwords = [line.strip() for line in open(stopfile, 'r', encoding='utf-8').readlines()]\n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取训练文件，并去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_without_stopwords(filename, stopfile):\n",
    "    \"\"\"读取文件数据\"\"\"\n",
    "    contents, labels = [], []\n",
    "    stopwords = stopwordslist(stopfile)\n",
    "    with open_file(filename) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                label, content = line.replace(' ', '').split('\\t')\n",
    "                if content:\n",
    "                    rawwords = list(jb.cut(native_content(content)))\n",
    "                    contents.append([_ for _ in rawwords if _ not in stopwords])\n",
    "                    labels.append(native_content(label))\n",
    "            except:\n",
    "                pass\n",
    "    return contents, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_words_table(trainfile, wordsfile, stopfile, vocab_size=5000):\n",
    "    \"\"\"根据训练集构建词汇表，存储\"\"\"\n",
    "    data_train, _ = read_file_without_stopwords(trainfile, stopfile)\n",
    "\n",
    "    all_data = []\n",
    "    for content in data_train:\n",
    "        all_data.extend(content)\n",
    "\n",
    "    counter = Counter(all_data)\n",
    "    count_pairs = counter.most_common(vocab_size - 1)\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    # 添加一个 <PAD> 来将所有文本pad为同一长度\n",
    "    words = ['<PAD>'] + list(words)\n",
    "    open_file(wordsfile, mode='w').write('\\n'.join(words) + '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载编码后的词表与种类表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vocab(vocab_dir):\n",
    "    \"\"\"读取词汇表\"\"\"\n",
    "    # words = open_file(vocab_dir).read().strip().split('\\n')\n",
    "    with open_file(vocab_dir) as fp:\n",
    "        # 如果是py2 则每个值都转化为unicode\n",
    "        words = [native_content(_.strip()) for _ in fp.readlines()]\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return words, word_to_id\n",
    "\n",
    "\n",
    "def read_category():\n",
    "    \"\"\"读取分类目录，固定\"\"\"\n",
    "    categories = ['体育', '财经', '房产', '家居', '教育', '科技', '时尚', '时政', '游戏', '娱乐']\n",
    "\n",
    "    categories = [native_content(x) for x in categories]\n",
    "\n",
    "    cat_to_id = dict(zip(categories, range(len(categories))))\n",
    "\n",
    "    return categories, cat_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文档编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_without_stopwords(filename, stopfile, word_to_id, cat_to_id, max_length=600):\n",
    "    \"\"\"将文件转换为id表示\"\"\"\n",
    "    contents, labels = read_file_without_stopwords(filename, stopfile)\n",
    "\n",
    "    data_id, label_id = [], []\n",
    "    for i in range(len(contents)):\n",
    "        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n",
    "        label_id.append(cat_to_id[labels[i]])\n",
    "\n",
    "    # 使用keras提供的pad_sequences来将文本pad为固定长度\n",
    "    # x_pad = kr.preprocessing.sequence.pad_sequences(data_id, max_length)\n",
    "    # y_pad = kr.utils.to_categorical(label_id, num_classes=len(cat_to_id))  # 将标签转换为one-hot表示\n",
    "\n",
    "    return data_id, label_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文档解码【每个词用/分隔开】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_words(content, words):\n",
    "    \"\"\"将id表示的内容转换为文字\"\"\"\n",
    "    return '/'.join(words[x] for x in content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 整个流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确定各个文件的路径\n",
    "trainfile = r'E:\\desktop\\task1\\cnews\\cnews.train.txt'\n",
    "wordfile = r'E:\\desktop\\task1\\cnews\\cnews.word.dict.txt'\n",
    "stopfile = r'E:\\desktop\\task1\\cnews\\stop.word.txt' \n",
    "\n",
    "# 生成词典表\n",
    "build_words_table(trainfile, wordfile, stopfile)\n",
    "\n",
    "# 加载词典表与种类表\n",
    "words, word_to_id = read_vocab(wordfile)\n",
    "_, cat_to_id = read_category()\n",
    "\n",
    "# 文档的编码\n",
    "x_pad, y_pad = process_file_without_stopwords(trainfile, stopfile, word_to_id, cat_to_id)\n",
    "\n",
    "#查看输出形状\n",
    "print(x_pad.shape)\n",
    "print(y_pad.shape)\n",
    "\n",
    "#一个具体的新闻文本的编码与解码输出\n",
    "print(x_pad[0])\n",
    "print(to_words(x_pad[0], words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jieba分词 统计词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jieba分词\n",
    "import jieba.posseg as pseg\n",
    "import collections\n",
    "\n",
    "words = pseg.cut(\"1、支持三种分词模式：(1)精确模式：试图将句子最精确的切开，适合文本分析。(2)全模式：把句子中所有可以成词的词语都扫描出来，速度非常快，但是不能解决歧义。(3)搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。2、支持繁体分词3、支持自定义词典\")\n",
    "for word in words:\n",
    "    print (word.word,word.flag)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词频统计\n",
    "print(\"-------------counter word--------------\")\n",
    "import collections\n",
    "word_counter = collections.Counter([word.text for word in seg_list])\n",
    "\n",
    "for k,v in word_counter.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本矩阵化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "import numpy as np\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    count = CountVectorizer()\n",
    "\n",
    "    docs_list=[]\n",
    "    mywordlist=[]\n",
    "    stopwords_path = \"resource/stop_words.txt\" # 停用词词表\n",
    "\n",
    "    # 读取文件\n",
    "    file_object = open('resource/text1.txt','r')\n",
    "    try:\n",
    "      for line in file_object:\n",
    "          # 文本分词\n",
    "          seg_list = jieba.cut(line, cut_all=False)\n",
    "          liststr=\"/ \".join(seg_list)\n",
    "\n",
    "           # 读取停用词文件\n",
    "          f_stop = open(stopwords_path,'r', encoding='UTF-8')\n",
    "          try:\n",
    "            f_stop_text = f_stop.read()\n",
    "          finally:\n",
    "            f_stop.close()\n",
    "\n",
    "          # 停用词清除\n",
    "          f_stop_seg_list = f_stop_text.split('\\n')\n",
    "          for myword in liststr.split('/'):\n",
    "            if not(myword.strip() in f_stop_seg_list) and len(myword.strip())>1:\n",
    "              mywordlist.append(myword)\n",
    "\n",
    "          docs_list.append(''.join(mywordlist))      # 存入文档列表中\n",
    "          mywordlist=[]                  # 存入之后，需要清除mywordlist内容，防止重复\n",
    "    finally:  \n",
    "      file_object.close()\n",
    "\n",
    "    print(f\"docs_list:{docs_list}\")\n",
    "\n",
    "    docs = np.array(docs_list)\n",
    "    print(f\"docs:{docs}\")\n",
    "    '''\n",
    "      output:\n",
    "      docs_list:[\n",
    "        '当地 时间 2017 15', '日本 神奈川县 横须贺', \n",
    "        ' 东芝 国际 反应堆 报废 研究 开发 机构 IRID 共同开发 机器人 公开 亮相', \n",
    "        '这个 30 厘米 直径 13 厘米 水下 机器人 投放 福岛 第一 核电站 机组 反应堆 安全壳 底部 展开 调查'\n",
    "      ]\n",
    "    docs:[\n",
    "      '当地 时间 2017 15' '日本 神奈川县 横须贺'\n",
    "      ' 东芝 国际 反应堆 报废 研究 开发 机构 IRID 共同开发 机器人 公开 亮相'\n",
    "      '这个 30 厘米 直径 13 厘米 水下 机器人 投放 福岛 第一 核电站 机组 反应堆 安全壳 底部 展开 调查'\n",
    "    ]\n",
    "  '''\n",
    "\n",
    "    #创建词袋模型的词汇库\n",
    "    bag = count.fit_transform(docs)\n",
    "    #查看词汇的位置，词汇是以字典的形式存储\n",
    "    print(count.vocabulary_)\n",
    "    '''\n",
    "      output:\n",
    "        {'当地': 16, '时间': 20, '2017': 2, '15': 1, '日本': 19, \n",
    "        '神奈川县': 29, '横须贺': 25, '东芝': 5, '国际': 11, \n",
    "        '反应堆': 10, '报废': 18, '研究': 28, '开发': 15, '机构': 22, \n",
    "        'irid': 4, '共同开发': 8, '机器人': 21, '公开': 7, '亮相': 6, \n",
    "        '这个': 33, '30': 3, '厘米': 9, '直径': 27, '13': 0, '水下': 26, \n",
    "        '投放': 17, '福岛': 30, '第一': 31, '核电站': 24, '机组': 23, \n",
    "        '安全壳': 12, '底部': 14, '展开': 13, '调查': 32}\n",
    "    '''\n",
    "    print(bag.toarray())\n",
    "    '''\n",
    "      output:\n",
    "        [[0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "       [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0]\n",
    "       [0 0 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0]\n",
    "       [1 0 0 1 0 0 0 0 0 2 1 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 1 1]]\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
